DATA_PATH = "data/dataset.json"

import json
with open(DATA_PATH, 'r', encoding='utf-8') as f:
    data = json.load(f)

print(f"–í—Å–µ–≥–æ —Ç–µ–∫—Å—Ç–æ–≤: {len(data)}")

class FRECalculator:
    @staticmethod
    def count_syllables_ru(word):
        return len(re.findall(r'[–∞–µ—ë–∏–æ—É—ã—ç—é—è]', word.lower()))
    @staticmethod
    def calculate(text):
        """–†–µ–∞–ª–∏–∑–∞—Ü–∏—è Flesch Reading Ease –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞"""
        words = re.findall(r'\w+', text)
        sentences = [s for s in re.split(r'(?<=[.!?]) +', text) if s]

        if not words or not sentences:
            return 0.0

        # –ü–æ–¥—Å—á–µ—Ç —Å–ª–æ–≥–æ–≤
        syllable_count = sum(FRECalculator.count_syllables_ru(w) for w in words)
        fre = 206.835 - (1.3 * (len(words)/len(sentences))) - (60.1 * (syllable_count/len(words)))
        return max(0, min(round(fre, 1), 100))
    
import re
import pymorphy3
import csv
morph = pymorphy3.MorphAnalyzer()
def load_abbreviations_from_csv(csv_path):
    abbreviation_dict = {}
    with open(csv_path, newline='', encoding='utf-8-sig') as csvfile:
        reader = csv.DictReader(csvfile, delimiter=';')  # –º–µ–Ω—è–µ–º –Ω–∞ ; !
        # print("–ó–∞–≥–æ–ª–æ–≤–∫–∏ –∫–æ–ª–æ–Ω–æ–∫:", reader.fieldnames)

        for row in reader:
            # print("–°—Ç—Ä–æ–∫–∞:", row)
            abbr = row["abbreviation"].strip()
            desc = row["description"].strip()
            if abbr and desc:
                abbreviation_dict[abbr] = desc

    return abbreviation_dict

class TextSimplifier:
    def __init__(self, abbreviation_dict=None, synonym_dict=None):
        self.abbreviation_dict = abbreviation_dict or {}
        self.synonym_dict = synonym_dict or {}
        self.abbreviation_dict = load_abbreviations_from_csv("./abbreviations.csv")
        # print('self.abbreviation_dict',self.abbreviation_dict)

    def simplify(self, text):
        lines = text.split('\n')
        cleaned_lines = [line.strip() for line in lines if line.strip()]
        text = ' '.join(cleaned_lines)
        text = self.expand_abbreviations(text)
        text = self.convert_passive_to_active(text)
        text = self.replace_complex_connectors(text)
        text = self.replace_complex_words(text)
        text = self.remove_redundancies(text)
        return text

    def expand_abbreviations(self, text):
        for abbr, full in self.abbreviation_dict.items():
            try:
                new_text = re.sub(abbr, full, text)
                # if new_text != text:
                    # print(f"üîÅ –ó–∞–º–µ–Ω–µ–Ω–æ: '{abbr}' ‚Üí '{full}'")
                text = new_text
            except re.error:
                continue  # –µ—Å–ª–∏ –Ω–µ–≤–∞–ª–∏–¥–Ω—ã–π regex ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
        return text

    def convert_passive_to_active(self, text):
        # –ü—Ä–∏–º–µ—Ä –ø—Ä–æ—Å—Ç–µ–π—à–µ–π –∑–∞–º–µ–Ω—ã –ø–∞—Å—Å–∏–≤–∞
        # –ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä–∏—Ç—å —Å –ø–æ–º–æ—â—å—é NLP-–±–∏–±–ª–∏–æ—Ç–µ–∫
        text = re.sub(r'–±—ã–ª[–∞–æ]? (\w{4,})(?:[–∞-—è]{2,3})?', r'\1', text)
        return text

    def replace_complex_connectors(self, text):
        replacements = {
            "–≤ —Å–≤—è–∑–∏ —Å —Ç–µ–º, —á—Ç–æ": "–ø–æ—Ç–æ–º—É —á—Ç–æ",
            "–≤ —Å–ª—É—á–∞–µ, –µ—Å–ª–∏": "–µ—Å–ª–∏",
            "–æ—Å—É—â–µ—Å—Ç–≤–ª—è–µ—Ç—Å—è": "–ø—Ä–æ–≤–æ–¥–∏—Ç—Å—è",
            "–Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏": "–∏–∑-–∑–∞",
            "–≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å": "—Å–æ–≥–ª–∞—Å–Ω–æ",
        }
        for old, new in replacements.items():
            pattern = r'\b' + re.escape(old) + r'\b'
            text = re.sub(pattern, new, text)
        return text

    def replace_complex_words(self, text):
        for complex_word, simple_word in self.synonym_dict.items():
            pattern = r'\b' + re.escape(complex_word) + r'\b'
            text = re.sub(pattern, simple_word, text)
        return text

    def remove_redundancies(self, text):
        redundancies = [
            r'\b–≤ —Ü–µ–ª–æ–º\b',
            r'\b—Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å\b',
            r'\b–≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è\b',
        ]
        for phrase in redundancies:
            text = re.sub(phrase, '', text)
        return text


LLM_URL = "http://localhost:9005/completions"
HEADERS = {"Content-Type": "application/json"}
MAX_TOKENS = 10000

import re
import requests
import json

class TextRefiner:
    def __init__(self):
        pass  # –ù–∏–∫–∞–∫–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è
    
    def clean_output(self, text):
        # –£–¥–∞–ª–∏—Ç—å –º–∞—Ä–∫–µ—Ä—ã –∏ –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        text = re.sub(r'^[-‚Ä¢\d\)\s]+', '', text, flags=re.MULTILINE)
        # –£–¥–∞–ª–∏—Ç—å –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏
        text = re.sub(r'\n{2,}', '\n', text)
        text = re.sub(r'\s{2,}', ' ', text)
        return text.strip()

    def refine(self, text):
        # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –Ω–∞ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
        sentences = re.split(r'(?<=[.!?])\s+', text.strip())
        
        simplified_paragraphs = []
        current_paragraph = []
        sentence_count = 0
        
        for sentence in sentences:
            # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–µ–Ω–µ–µ 20 —Å–∏–º–≤–æ–ª–æ–≤
            match = re.search(r'(\b\w{5,})[.!?]$', sentence)
            if len(sentence.strip()) >= 70 or match:
                current_paragraph.append(sentence.strip())
                sentence_count += 1
            else:
                current_paragraph.append(sentence.strip())
                
            
            # –ï—Å–ª–∏ —Å–æ–±—Ä–∞–ª–∏ 4 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –∏—Ö –Ω–∞ –æ–±—Ä–∞–±–æ—Ç–∫—É
            if sentence_count == 7:
                paragraph = ' '.join(current_paragraph)
                simplified_paragraphs.append(self.process_paragraph(paragraph))
                current_paragraph = []
                sentence_count = 0
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Å—Ç–∞–≤—à–∏—Ö—Å—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π, –µ—Å–ª–∏ –µ—Å—Ç—å
        if current_paragraph:
            paragraph = ' '.join(current_paragraph)
            simplified_paragraphs.append(self.process_paragraph(paragraph))
        
        return ' '.join(simplified_paragraphs)

    def process_paragraph(self, paragraph):

        # prompt = f"""
        # –¢—ã ‚Äî –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç, –∫–æ—Ç–æ—Ä—ã–π –ø–µ—Ä–µ–ø–∏—Å—ã–≤–∞–µ—Ç —Å–ª–æ–∂–Ω—ã–µ –¥–µ–ª–æ–≤—ã–µ —Ç–µ–∫—Å—Ç—ã –ø—Ä–æ—Å—Ç—ã–º –∏ –ø–æ–Ω—è—Ç–Ω—ã–º —è–∑—ã–∫–æ–º.

        # –¢–≤–æ—è —Ü–µ–ª—å ‚Äî —Å–¥–µ–ª–∞—Ç—å —Ç–µ–∫—Å—Ç —É–¥–æ–±–Ω—ã–º –¥–ª—è —á—Ç–µ–Ω–∏—è, –ø–æ–Ω—è—Ç–Ω—ã–º –∏ –¥–æ—Å—Ç—É–ø–Ω—ã–º —á–µ–ª–æ–≤–µ–∫—É –±–µ–∑ —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ–≥–æ –æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è. –ü–∏—à–∏ —Ç–∞–∫, –∫–∞–∫ –±—É–¥—Ç–æ –æ–±—ä—è—Å–Ω—è–µ—à—å –¥—Ä—É–≥—É. –ò–∑–±–µ–≥–∞–π —Å—É—Ö–æ–≥–æ –¥–µ–ª–æ–≤–æ–≥–æ —Å—Ç–∏–ª—è.

        # üîπ –°—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞:

        # 1. –ü–∏—à–∏ –∫–æ—Ä–æ—Ç–∫–∏–º–∏ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è–º–∏ ‚Äî –¥–æ 12‚Äì15 —Å–ª–æ–≤.
        # 2. –£–ø—Ä–æ—â–∞–π —Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∫–∏. –ó–∞–º–µ–Ω—è–π —Å–ª–æ–∂–Ω—ã–µ —Å–ª–æ–≤–∞ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ —Å–∏–Ω–æ–Ω–∏–º—ã.
        # 3. –†–∞–∑–¥–µ–ª—è–π –≥—Ä–æ–º–æ–∑–¥–∫–∏–µ —Ñ—Ä–∞–∑—ã –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ –±–ª–æ–∫–∏.
        # 4. –ù–µ –¥–æ–±–∞–≤–ª—è–π –Ω–∏—á–µ–≥–æ –æ—Ç —Å–µ–±—è. –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–ø–∏—à–∏ –ø–æ–Ω—è—Ç–Ω–µ–µ.
        # 5. –°–æ—Ö—Ä–∞–Ω—è–π –ª–æ–≥–∏—á–µ—Å–∫—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É —Ç–µ–∫—Å—Ç–∞.
        # 6. –ò–∑–±–µ–≥–∞–π –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤ (¬´–≤ —Ü–µ–ª–æ–º¬ª, ¬´—Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å¬ª –∏ —Ç.–ø.).
        # 7. –£–±–∏—Ä–∞–π –ø–∞—Å—Å–∏–≤–Ω—ã–µ –∫–æ–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏. –ò—Å–ø–æ–ª—å–∑—É–π ¬´–∫—Ç–æ –¥–µ–ª–∞–µ—Ç —á—Ç–æ¬ª.
        # 8. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π –±—é—Ä–æ–∫—Ä–∞—Ç–∏—á–µ—Å–∫–∏–π —Å—Ç–∏–ª—å.

        # üìà –¶–µ–ª—å ‚Äî –¥–æ–±–∏—Ç—å—Å—è –≤—ã—Å–æ–∫–æ–π –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏ —Ç–µ–∫—Å—Ç–∞ –∏ –Ω–∏–∑–∫–æ–π —Å–ª–æ–∂–Ω–æ—Å—Ç–∏ –ø–æ —à–∫–∞–ª–µ —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏.

        # –ü—Ä–∏–º–µ—Ä:
        # –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –í —Ä–∞–º–∫–∞—Ö –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –±—ã–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –º–µ—Ä—ã –ø–æ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ä—ã–Ω–∫–∞.
        # –ü–æ–Ω—è—Ç–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç: –ó–∞–ø—É—Å—Ç–∏–ª–∏ –ø—Ä–æ–µ–∫—Ç. –û–Ω –ø–æ–º–æ–≥ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä—ã–Ω–æ–∫.

        # –¢–µ–ø–µ—Ä—å –ø–µ—Ä–µ–ø–∏—à–∏ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç –ø—Ä–æ—â–µ –∏ –ø–æ–Ω—è—Ç–Ω–µ–µ:
        # {paragraph.strip()}
        # """

        prompt = f"""
        –¢—ã ‚Äî –ø–æ–º–æ—â–Ω–∏–∫, –∫–æ—Ç–æ—Ä—ã–π —É–ø—Ä–æ—â–∞–µ—Ç —Ç–µ–∫—Å—Ç—ã –ø–æ –ø—Ä–∞–≤–∏–ª–∞–º —è—Å–Ω–æ–≥–æ –∏ –ø—Ä–æ—Å—Ç–æ–≥–æ —è–∑—ã–∫–∞.

        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ ‚Äî –ø–µ—Ä–µ–ø–∏—Å–∞—Ç—å —Ç–µ–∫—Å—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ –ø—Ä–æ—Å—Ç–æ, —Ç–∞–∫ —á—Ç–æ–±—ã –æ–Ω –ø–æ–¥—Ö–æ–¥–∏–ª –¥–∞–∂–µ –¥–ª—è –º–ª–∞–¥—à–∏—Ö —à–∫–æ–ª—å–Ω–∏–∫–æ–≤. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Å–∞–º—ã–µ –ø—Ä–æ—Å—Ç—ã–µ –∏ –∑–Ω–∞–∫–æ–º—ã–µ —Å–ª–æ–≤–∞.

        –°—Ç—Ä–æ–≥–æ —Å–æ–±–ª—é–¥–∞–π —Å–ª–µ–¥—É—é—â–∏–µ –ø—Ä–∞–≤–∏–ª–∞:

        1. –†–∞–∑–±–∏–≤–∞–π —Å–ª–æ–∂–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –Ω–∞ 2‚Äì3 –∫–æ—Ä–æ—Ç–∫–∏—Ö. –ù–µ –¥–µ–ª–∞–π –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–ª–∏–Ω–Ω–µ–µ 12 —Å–ª–æ–≤.
        2. –ò–∑–±–µ–≥–∞–π —Ä–µ–¥–∫–∏—Ö, –¥–ª–∏–Ω–Ω—ã—Ö –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω—ã—Ö —Å–ª–æ–≤.
        3. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ –æ–±—â–µ—É–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –∏–∑ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –ª–µ–∫—Å–∏–∫–æ–Ω–∞.
        4. –ï—Å–ª–∏ –µ—Å—Ç—å —Å–ª–æ–∂–Ω—ã–µ —Ñ—Ä–∞–∑—ã –∏–ª–∏ –æ–±–æ—Ä–æ—Ç—ã ‚Äî –∑–∞–º–µ–Ω–∏ –∏—Ö –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ —Å–ª–æ–≤–∞.
        5. –ü–æ–≤—Ç–æ—Ä—ã –¥–æ–ø—É—Å—Ç–∏–º—ã. –ì–ª–∞–≤–Ω–æ–µ ‚Äî —á—Ç–æ–±—ã –±—ã–ª–æ –ø–æ–Ω—è—Ç–Ω–æ.
        6. –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –∫–∞–∂–¥–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–æ—Å—Ç–æ–π: –ø–æ–¥–ª–µ–∂–∞—â–µ–µ + —Å–∫–∞–∑—É–µ–º–æ–µ + –¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ.
        7. –ù–µ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π, –Ω–µ —Ä–∞—Å—Å—É–∂–¥–∞–π, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –Ω–æ–≤–æ–≥–æ.
        8. –ù–µ –æ–±—ä—è—Å–Ω—è–π —Å–º—ã—Å–ª. –ü—Ä–æ—Å—Ç–æ –ø–µ—Ä–µ–ø–∏—à–∏ –ø—Ä–æ—â–µ.
        9. –ò–∑–±–µ–≥–∞–π –≤–≤–æ–¥–Ω—ã—Ö —Å–ª–æ–≤: ¬´–≤ —Ü–µ–ª–æ–º¬ª, ¬´—Å–ª–µ–¥—É–µ—Ç –æ—Ç–º–µ—Ç–∏—Ç—å¬ª, ¬´–≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è¬ª.
        10. **–ö–∞–∂–¥–æ–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –∫–æ—Ä–æ—Ç–∫–∏–º –∏ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω—ã–º.**

        –¢–≤–æ—è —Ü–µ–ª—å: —á—Ç–æ–±—ã —É —Ç–µ–∫—Å—Ç–∞ –±—ã–ª–∞ –∫–∞–∫ –º–æ–∂–Ω–æ **–Ω–∏–∂–µ —Å–ª–æ–∂–Ω–æ—Å—Ç—å** –ø–æ —à–∫–∞–ª–µ ARI –∏ –∫–∞–∫ –º–æ–∂–Ω–æ **–≤—ã—à–µ –æ—Ü–µ–Ω–∫–∞ –ø—Ä–æ–∑—Ä–∞—á–Ω–æ—Å—Ç–∏** –ø–æ —à–∫–∞–ª–µ Alina.

        –ü—Ä–∏–º–µ—Ä:
        –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: –í —Ä–∞–º–∫–∞—Ö –æ—Å—É—â–µ—Å—Ç–≤–ª–µ–Ω–∏—è –ø—Ä–æ–µ–∫—Ç–∞ –±—ã–ª–∏ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω—ã –º–µ—Ä—ã –ø–æ —Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏–∏ —Ä—ã–Ω–∫–∞.
        –û—Ç–≤–µ—Ç: –ü—Ä–æ–µ–∫—Ç –∑–∞–ø—É—Å—Ç–∏–ª–∏. –û–Ω –ø–æ–º–æ–≥ —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ä—ã–Ω–æ–∫.

        –¢–µ–ø–µ—Ä—å —É–ø—Ä–æ—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Ç–µ–∫—Å—Ç:
        {paragraph.strip()}
        """

        payload = {
            "prompt": prompt,
            "max_tokens": 512,
            "temperature": 0.7,
            "top_k": 10
        }
        print(f"\n=== –û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –¥–ª—è –±–ª–æ–∫–∞ ===")
        print(f"–¢–µ–∫—Å—Ç:\n{prompt}\n")
        try:
            response = requests.post(LLM_URL, headers=HEADERS, data=json.dumps(payload))
            response.raise_for_status()
            
            result = response.json()

            content = result.get("content", "").strip()
                        

            lines = content.splitlines()
            cleaned_lines = []

            for i, line in enumerate(lines):
                stripped = line.strip()

                if not stripped:
                    continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Ç—Ä–æ–∫–∏

                if i == 0:
                    # –£–¥–∞–ª—è–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞, –æ–¥–∏–Ω–æ–∫–∏–µ —Å–∏–º–≤–æ–ª—ã –∏ –ª–∏—à–Ω–∏–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –∏–∑ –ø–µ—Ä–≤–æ–π —Å—Ç—Ä–æ–∫–∏
                    cleaned_line = re.sub(r'^(?:[^\w–∞-—è–ê-–Ø]*\b)?(–û—Ç–≤–µ—Ç|Answer|output|–£–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç)?[:\-\s_#>]*', '', stripped, flags=re.IGNORECASE)
                    
                    # –ï—Å–ª–∏ –ø–æ—Å–ª–µ —É–¥–∞–ª–µ–Ω–∏—è –≤—Å—ë —Ä–∞–≤–Ω–æ –æ—Å—Ç–∞–ª—Å—è —Ç–æ–ª—å–∫–æ –æ–¥–∏–Ω–æ—á–Ω—ã–π —Å–∏–º–≤–æ–ª ‚Äî —É–±–∏—Ä–∞–µ–º
                    if len(cleaned_line.strip()) <= 2:
                        continue
                else:
                    # –£–±–∏—Ä–∞–µ–º —Å—Ç—Ä–æ–∫–∏ –∏–∑ –æ–¥–Ω–æ–≥–æ —Å–∏–º–≤–æ–ª–∞
                    if len(stripped) <= 2:
                        continue
                    cleaned_line = stripped

                cleaned_lines.append(cleaned_line)

            # –°–æ–±–∏—Ä–∞–µ–º —Ç–µ–∫—Å—Ç –≤ –æ–¥–Ω—É —Å—Ç—Ä–æ–∫—É
            cleaned = " ".join(cleaned_lines).strip()

            # cleaned = self.clean_output(cleaned)
            cleaned = " ".join([line.strip() for line in cleaned.splitlines() if line.strip()])

            print(f"üì• –û—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏:\n{cleaned}\n")
            return cleaned
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –±–ª–æ–∫–∞: {e}")
            return paragraph  # fallback: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    
import requests
def get_score(model_id, text, auth_token= '4e5f6a7b-8c9d-0e1f-2a3b-4c5d6e7f8a9b'):
  payload = {'model_id':model_id, 'text':text}
  headers = {'Authorization': auth_token}
  resp = requests.post('http://skolkovo.cbrai.ru/api/v1/score', json=payload, headers=headers)
  if resp.status_code == 200:
    return resp.json() ['score']
  else:
    print (f"Error acquired: {resp.status_code}, {resp.text}" )
    return "Error"
  
class Pipeline:
    def __init__(self):
        self.simplifier = TextSimplifier()
        self.refiner = TextRefiner()
        self.fre = FRECalculator()

    def process(self, text):
        # print("=== –ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç ===\n", text + "\n")

        # –û—Ü–µ–Ω–∫–∞ –∫–∞—á–µ—Å—Ç–≤–∞
        orig_metrics = {
            'Alina': get_score('AlinaEstimator', (text)),
            'FRE': self.fre.calculate(text),
            'Arie': get_score('ARIEstimator',(text))
        }


        simplified = self.simplifier.simplify(text)
        intermediate = simplified
        refined = self.refiner.refine(simplified)


        refined_metrics = {
            'Alina': get_score('AlinaEstimator',(refined)),
            'FRE': self.fre.calculate(refined),
            'Arie': get_score('ARIEstimator',(refined))
        }

        return {
            'text': refined,
            'simplified': intermediate,
            'metrics': {
                'original': orig_metrics,
                'simplified': refined_metrics
            }
        }
    
pipeline = Pipeline()

alina_scores_before = []
alina_scores_after = []
arie_scores_before = []
arie_scores_after = []
fre_scores_before = []
fre_scores_after = []

with open("results.txt", "w", encoding="utf-8") as f:
    i = 0  # —Å—á—ë—Ç—á–∏–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤

    for key, item in data.items():
        orig_alina = item.get("AlinaEstimator", 0)

        if orig_alina >= 2:
            continue  # –ø—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã —Å –º–µ—Ç—Ä–∏–∫–æ–π Alina >= 3

        text = item['text']
        result = pipeline.process(text)

        # –ú–µ—Ç—Ä–∏–∫–∏
        simp_alina = result['metrics']['simplified']['Alina']
        orig_arie = result['metrics']['original']['Arie']
        simp_arie = result['metrics']['simplified']['Arie']
        orig_fre = result['metrics']['original']['FRE']
        simp_fre = result['metrics']['simplified']['FRE']

        # –î–æ–±–∞–≤–ª—è–µ–º —É–ø—Ä–æ—â—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –≤ —Ç–µ–∫—É—â–∏–π —ç–ª–µ–º–µ–Ω—Ç —Å–ª–æ–≤–∞—Ä—è
        item['easy_text'] = result['text']

        # –î–ª—è –ø–æ–¥—Å—á—ë—Ç–∞ —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
        alina_scores_before.append(orig_alina)
        alina_scores_after.append(simp_alina)
        arie_scores_before.append(orig_arie)
        arie_scores_after.append(simp_arie)
        fre_scores_before.append(orig_fre)
        fre_scores_after.append(simp_fre)

        i += 1
        # if i >= 10:
        #     break  # —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ 10 –ø–æ–¥—Ö–æ–¥—è—â–∏—Ö
        print(i)
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –Ω–æ–≤—ã–π —Ñ–∞–π–ª
    with open("data/updated_dataset.json", "w", encoding="utf-8") as f_out:
        json.dump(data, f_out, ensure_ascii=False, indent=2)

    # –í—ã–≤–æ–¥ —Å—Ä–µ–¥–Ω–∏—Ö –º–µ—Ç—Ä–∏–∫
    def avg(lst):
        try:
            numeric = [float(x) for x in lst]
            return round(sum(numeric) / len(numeric), 2) if numeric else 0.0
        except ValueError:
            return 0.0

    print("\n========== –°–†–ï–î–ù–ò–ï –ú–ï–¢–†–ò–ö–ò ==========")
    print(f"Alina: {avg(alina_scores_before)} ‚Üí {avg(alina_scores_after)}")
    print(f"Arie: {avg(arie_scores_before)} ‚Üí {avg(arie_scores_after)}")
    print(f"FRE: {avg(fre_scores_before)} ‚Üí {avg(fre_scores_after)}")
